---
title: 'Introduction to tidymodels: Consistent and Modular Modeling in R'
author: "Piotr Kosowski"
date: "2025-04-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction to `tidymodels`

`tidymodels` is an **ecosystem of R packages** designed for modeling and machine learning that adheres to the **tidyverse** design principles. Instead of a single monolithic package, `tidymodels` comprises many smaller, interoperable packages, each focusing on a specific stage of the modeling process. This approach promotes consistency, modularity, and ease in creating, evaluating, and deploying models.

**Main goals of `tidymodels`:**

1.  **Consistent Interface:** Provide a unified way to interact with various model types and R packages.
2.  **Modularity:** Break down the modeling process into logical steps (e.g., data splitting, preprocessing, model specification, evaluation), making the workflow easier to manage and modify.
3.  **Focus on Good Practices:** Facilitate the application of sound methodologies, such as proper data splitting, cross-validation, and avoiding data leakage.
4.  **Integration with Tidyverse:** Seamlessly work with other `tidyverse` packages like `dplyr` for data manipulation and `ggplot2` for visualization.

This notebook will guide you through the `tidymodels` philosophy, introduce its key packages, and demonstrate how to build a complete ML modeling workflow.

------------------------------------------------------------------------

## Setting Up the Environment

Before we begin, let's ensure the necessary packages are installed and loaded. The main package is `tidymodels`, which automatically installs and loads the core packages of the ecosystem. We will also use the `palmerpenguins` dataset as an example.

```{r install-load-libs}
# Define required libraries
libs <- c("tidymodels", "palmerpenguins", "tidyverse") # tidymodels loads dplyr, ggplot2 etc.

# Check if libraries are installed; install missing ones
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
  install.packages(libs[!installed_libs])
}

# Load libraries
library(tidymodels)
library(palmerpenguins)
library(tidyverse) # For additional tools, e.g., data cleaning

# Prepare data - remove missing values for simplicity
penguins_data <- penguins %>% 
  filter(
    !is.na(bill_length_mm),
    !is.na(flipper_length_mm),
    !is.na(sex)
    ) %>% 
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, island) # Select relevant columns

# Set random seed for reproducibility
set.seed(123) 
```

**Explanation:** - **`libs`**: A vector of required package names. - **Installation Check**: The code checks if packages are installed and installs any missing ones. - **`library()`**: Loads the packages into the R session. `tidymodels` loads many other packages (like `rsample`, `recipes`, `parsnip`, `workflows`, `yardstick`, `tune`, `dials`). - **`palmerpenguins`**: A dataset about penguins, which we will use for demonstration. - **Data Cleaning**: We remove rows with missing values (`NA`) in key columns to simplify the example. - **`set.seed(123)`**: Ensures that random operations (like data splitting) are reproducible.

------------------------------------------------------------------------

## The `tidymodels` Philosophy: Step-by-Step

The modeling process in `tidymodels` is broken down into consistent, logical steps. Understanding this workflow is key:

1.  **Data Splitting (`rsample`):**
    -   **Goal:** Divide the original dataset into a training set (for model building) and a testing set (for final evaluation). Cross-validation is often used on the training set for hyperparameter tuning and assessing model stability.
    -   **Why?** Prevents model overfitting and provides a realistic estimate of its performance on new, unseen data.
    -   **Key functions:** `initial_split()`, `training()`, `testing()`, `vfold_cv()`.
2.  **Preprocessing (`recipes`):**
    -   **Goal:** Define a sequence of data preparation steps (a *recipe*) that will be applied *only* to the training data (to avoid data leakage to the test set). The same transformations are then applied to the test data. Steps can include imputing missing values, creating dummy variables, normalization, transformations (e.g., logarithmic), principal component analysis (PCA), etc.
    -   **Why?** Automates and standardizes data preparation, ensuring consistency and preventing errors. Recipes are defined *before* being applied to data.
    -   **Key functions:** `recipe()`, `step_*()` (e.g., `step_dummy()`, `step_normalize()`, `step_impute_mean()`, `step_pca()`).
3.  **Model Specification (`parsnip`):**
    -   **Goal:** Define the *type* of model (e.g., linear regression, random forest, SVM), its *engine* (the specific R package implementation, e.g., `lm`, `ranger`, `kernlab`), and its *mode* (e.g., regression, classification). `parsnip` provides a consistent interface to different model implementations.
    -   **Why?** Separates the model definition from its fitting and implementation, making it easy to experiment with different models and engines without changing the rest of the code.
    -   **Key functions:** `linear_reg()`, `logistic_reg()`, `rand_forest()`, `boost_tree()`, `set_engine()`, `set_mode()`.
4.  **Bundling into a Workflow (`workflows`):**
    -   **Goal:** Combine the model specification (`parsnip`) and the data processing recipe (`recipes`) into a single `workflow` object.
    -   **Why?** Simplifies managing the modeling process. Ensures that preprocessing is applied correctly during training, prediction, and cross-validation.
    -   **Key functions:** `workflow()`, `add_recipe()`, `add_model()`.
5.  **Model Training and Tuning (`tune`, `dials`):**
    -   **Goal:** Fit the `workflow` to the training data (`fit()`) or, if the model has hyperparameters to optimize (e.g., number of trees in a random forest), use cross-validation (defined in step 1) to find the best values for these hyperparameters (`tune_grid()`, `tune_bayes()`). The `dials` package helps manage the hyperparameter space.
    -   **Why?** Hyperparameter optimization helps achieve the best possible model performance. Cross-validation provides a robust evaluation.
    -   **Key functions:** `fit()`, `tune_grid()`, `tune_bayes()`, `dials::grid_regular()`, `select_best()`, `finalize_workflow()`.
6.  **Model Evaluation (`yardstick`):**
    -   **Goal:** Assess the performance of the trained model (or tuning results) using appropriate metrics (e.g., RMSE for regression; Accuracy, AUC, F1-score for classification).
    -   **Why?** Allows objective comparison of different models or hyperparameter configurations to select the best model for the task.
    -   **Key functions:** `metrics()`, `metric_set()`, `conf_mat()`, `autoplot()`.
7.  **Prediction on New Data:**
    -   **Goal:** Use the final, trained `workflow` to generate predictions on the test set (held out in step 1) or on entirely new data.
    -   **Why?** This is the ultimate goal of modeling â€“ using the model to predict outcomes for data the model hasn't seen before.
    -   **Key functions:** `predict()`, `augment()`.

This structured workflow is the heart of `tidymodels` and distinguishes it from more ad-hoc modeling approaches.

------------------------------------------------------------------------

## Key `tidymodels` Packages

Let's take a closer look at the most important packages within the `tidymodels` ecosystem:

### 1. `rsample`: Data Splitting and Resampling

-   **Purpose:** Creating training/testing datasets and resampling schemes (e.g., cross-validation, bootstrap).
-   **Example functions:**
    -   `initial_split(data, prop, strata)`: Splits data into training and testing sets (prop - proportion for training, strata - stratify by a variable).
    -   `training(split)`: Extracts the training set from a `split` object.
    -   `testing(split)`: Extracts the testing set from a `split` object.
    -   `vfold_cv(data, v, strata)`: Creates V-fold cross-validation folds (v - number of folds).

### 2. `recipes`: Data Preprocessing

-   **Purpose:** Defining a sequence of data transformation steps that can be applied consistently to different datasets.
-   **Example functions:**
    -   `recipe(formula, data)`: Initializes a recipe, defining variable roles (outcome \~ predictors).
    -   `step_dummy(recipe, all_nominal_predictors())`: Converts categorical variables to dummy/indicator variables.
    -   `step_normalize(recipe, all_numeric_predictors())`: Standardizes (centers and scales) numeric variables.
    -   `step_impute_mean(recipe, ...)`: Imputes missing values using the mean.
    -   `prep(recipe)`: Estimates parameters needed for recipe steps from the training data (e.g., means, standard deviations for normalization).
    -   `bake(prep_recipe, new_data)`: Applies the prepared recipe to new data.

### 3. `parsnip`: Model Specification

-   **Purpose:** Providing a unified interface for defining models, regardless of the underlying R package that implements them.
-   **Example functions:**
    -   `linear_reg()`: Defines a linear regression model.
    -   `logistic_reg()`: Defines a logistic regression model.
    -   `rand_forest()`: Defines a random forest model.
    -   `set_engine(model_spec, "engine_name", ...)`: Specifies the R package (engine) to be used for training (e.g., `"lm"`, `"ranger"`, `"glmnet"`).
    -   `set_mode(model_spec, "regression" | "classification")`: Specifies the modeling mode.

### 4. `workflows`: Bundling the Workflow

-   **Purpose:** Bundling a `recipe` and a `parsnip` model specification into a single workflow object.
-   **Example functions:**
    -   `workflow()`: Creates an empty workflow.
    -   `add_recipe(workflow, recipe)`: Adds a recipe to the workflow.
    -   `add_model(workflow, model_spec)`: Adds a model specification to the workflow.

### 5. `tune`: Hyperparameter Tuning

-   **Purpose:** Optimizing model hyperparameters using resampling methods (e.g., cross-validation).
-   **Example functions:**
    -   `tune_grid(workflow, resamples, grid, metrics)`: Searches a grid of hyperparameters, evaluating the model on `resamples` objects.
    -   `fit_resamples(workflow, resamples, metrics)`: Fits a model (without tuning) on `resamples` objects to evaluate its performance.
    -   `select_best(tuning_results, metric)`: Selects the best hyperparameter combination based on a metric.
    -   `finalize_workflow(workflow, best_params)`: Updates the workflow by setting the best found hyperparameters.

### 6. `dials`: Managing Hyperparameters

-   **Purpose:** Defining and managing hyperparameter spaces for tuning.
-   **Example functions:**
    -   `parameters(object)`: Identifies parameters (including tunable ones) in a recipe or model.
    -   `grid_regular(params, levels)`: Creates a regular grid of hyperparameters.
    -   `mtry()`, `trees()`, `penalty()`: Functions defining specific hyperparameters and their ranges.

### 7. `yardstick`: Model Evaluation

-   **Purpose:** Calculating and visualizing model performance metrics.
-   **Example functions:**
    -   `metrics(data, truth, estimate)`: Calculates a default set of metrics based on true values (`truth`) and predictions (`estimate`).
    -   `metric_set(metric1, metric2, ...)`: Creates a custom set of metrics to calculate.
    -   `accuracy()`, `rmse()`, `roc_auc()`, `f_meas()`: Functions for specific metrics.
    -   `conf_mat(data, truth, estimate)`: Calculates a confusion matrix.
    -   `autoplot(object)`: Automatically generates visualizations for `tune` or `yardstick` results (e.g., ROC curves).

------------------------------------------------------------------------

## Building a Model Step-by-Step: `palmerpenguins` Example

Let's apply the concepts learned to build a classification model that predicts penguin `species` based on their measurements and sex.

### Step 1: Data Splitting (`rsample`)

We'll split the `penguins_data` into training (75%) and testing (25%) sets, using stratification by `species` to maintain similar proportions of species in both sets.

```{r}
# Split data into training (3/4) and testing (1/4) with stratification
penguin_split <- initial_split(penguins_data, prop = 3/4, strata = species)

# Extract the sets
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)

# Information about the split
cat("Training data:", nrow(penguin_train), "observations\n")
cat("Testing data:", nrow(penguin_test), "observations\n")

# Create cross-validation folds (10-fold) on the training data
penguin_folds <- vfold_cv(penguin_train, v = 10, strata = species)
```

### Step 2: Preprocessing (`recipes`)

We'll define a simple recipe: 1. Specify the formula: `species` as the outcome, all others as predictors. 2. Convert categorical predictors (`sex`, `island`) to dummy variables. 3. Normalize (center and scale) all numeric predictors.

```{r}
penguin_recipe <- recipe(
    species ~ .,
    data = penguin_train) %>% 
  step_normalize(all_numeric_predictors()) %>%    # Normalize numeric predictors
  step_dummy(all_nominal_predictors(), -all_outcomes()) # Create dummies for 'sex' and 'island'

```

```{r}
# You can view a summary of the recipe
summary(penguin_recipe)
```

```{r}
# To see what the recipe would do to the data:
prep_recipe <- prep(penguin_recipe, training = penguin_train)
bake(prep_recipe, new_data = head(penguin_train))
```

**Explanation:** - `recipe(species ~ ., data = penguin_train)`: Initializes the recipe. `species` is the outcome variable, and `.` means all other columns are predictors. `penguin_train` data is used to determine variable types etc. - `step_dummy(all_nominal_predictors(), -all_outcomes())`: Creates dummy variables for all nominal (categorical) predictors. We exclude the outcome variable (`-all_outcomes()`). - `step_normalize(all_numeric_predictors())`: Centers and scales all numeric predictors.

### Step 3: Model Specification (`parsnip`)

We'll define a random forest model (`rand_forest`) for the classification task. We'll use the `"ranger"` engine, which is a fast implementation of random forests. For now, we won't tune hyperparameters; we'll use default values.

```{r}
# Specification for a random forest model for classification
rf_spec <- rand_forest(mode = "classification") %>% 
  set_engine("ranger") # Use the 'ranger' engine

rf_spec
```

### Step 4: Bundling into a Workflow (`workflows`)

We'll combine our recipe (`penguin_recipe`) and model specification (`rf_spec`) into a single `workflow`.

```{r}
# Create the workflow
penguin_wf <- workflow() %>% 
  add_recipe(penguin_recipe) %>% 
  add_model(rf_spec)

penguin_wf
```

### Step 5: Training the Model (without tuning for now)

We'll use the `fit()` function to train our `workflow` on the training data. The workflow automatically applies the recipe steps before training the model.

```{r fit-model}
# Train the workflow on the training data
penguin_fit <- fit(penguin_wf, data = penguin_train)
```

```{r}
# Display the fitted model (information from the ranger engine)
penguin_fit
```

```{r}
# You can also extract the model object itself:
extract_fit_parsnip(penguin_fit)
```

### Step 6: Evaluating the Model (`yardstick`) - on the Test Set

Now, we'll use the trained model (`penguin_fit`) to generate predictions on the test set (`penguin_test`) and evaluate its performance using `yardstick` metrics.

```{r}
# Generate predictions: class
predictions_class <- predict(penguin_fit, new_data = penguin_test, type = "class")
predictions_class
```

```{r}
# Generate predictions: probabilities
predictions_prob <- predict(penguin_fit, new_data = penguin_test, type = "prob")
predictions_prob
```

```{r}
# 2. Combine predictions with true values
# Make sure the true 'species' column is included
results_df <- bind_cols(
    predictions_class,  # Includes .pred_class
    predictions_prob, # Includes .pred_Adelie, .pred_Chinstrap, .pred_Gentoo
    penguin_test %>%
      select(species) # Add the true species column
  )

results_df
```

```{r}
# Define metrics - accuracy and multiclass AUC (macro-averaged)
classification_metrics <- metric_set(
  accuracy, 
  roc_auc,
  precision,
  recall)
classification_metrics 
```

```{r}
# Calculate metrics

eval_results <- classification_metrics(
    results_df,
    truth = species,
    estimate = .pred_class, # For accuracy,
    .pred_Adelie, .pred_Chinstrap, .pred_Gentoo, # For AUC
    estimator = "macro" 
  )

# 5. Display the results
print(eval_results)
```

# 6. Calculate and plot confusion matrix (uses .pred_class)

# This part should still work as before

```{r}
conf_mat(results_df, truth = species, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

**Explanation:** - `predict(penguin_fit, new_data = penguin_test)`: Generates predicted classes for the test data. - `bind_cols()`: Combines the prediction column with the true value column. - `metric_set(accuracy, roc_auc)`: Defines the set of metrics we want to calculate. - `metrics(...)`: Calculates the defined metrics. `truth` is the column with true values, `estimate` is the column with predictions. `.pred_class` is the default name for the predicted class column. `estimator="macro"` is needed for ROC AUC in multiclass problems. - `conf_mat(...) %>% autoplot()`: Calculates and visualizes the confusion matrix.

### (Optional) Step 5b: Hyperparameter Tuning (`tune`)

Let's assume we want to optimize the `mtry` hyperparameter (number of variables randomly sampled at each split) in our random forest.

```{r}
# eval=FALSE because it might take a while
# 1. Update the model specification to mark 'mtry' for tuning
rf_spec_tune <- rand_forest(
  mtry = tune(),
  trees = 500,
  mode = "classification") %>% # tune() marks hyperparameter for tuning
  set_engine("ranger")
```

```{r}
# 2. Update the workflow
penguin_wf_tune <- workflow() %>% 
  add_recipe(penguin_recipe) %>% 
  add_model(rf_spec_tune)
```

```{r}
# 3. Define a grid of 'mtry' values to test (dials helps define the range)
# First, get the mtry range from 'dials' based on the data
mtry_param <- mtry(
  range = c(1, ncol(penguin_train) - 1) # mtry from 1 to number of predictors
  ) 

# Create a grid of e.g., 5 mtry values
rf_grid <- grid_regular(mtry_param, levels = 5) 
```

```{r}
# 4. Perform tuning using cross-validation (penguin_folds)
tune_res <- tune_grid(
  penguin_wf_tune,        # Workflow with tunable model
  resamples = penguin_folds, # Cross-validation folds
  grid = rf_grid,         # Hyperparameter grid
  metrics = metric_set(accuracy, roc_auc) # Metrics for evaluation
)
```

```{r}
# 5. Display tuning results
show_best(tune_res, metric = "roc_auc")
```

```{r}
autoplot(tune_res) # Visualize tuning results
```

```{r}
# 6. Select the best mtry value
best_mtry <- select_best(tune_res, metric = "roc_auc")
best_mtry
```

```{r}
# 7. Finalize the workflow with the best parameters
final_rf_wf <- finalize_workflow(penguin_wf_tune, best_mtry)
final_rf_wf
```

```{r}
# 8. Train the final model on the *entire* training set
final_rf_fit <- fit(final_rf_wf, data = penguin_train)

```

```{r}
# 9. Evaluate the final model on the test set (as in step 6)
final_predictions_class <- predict(final_rf_fit, new_data = penguin_test, type = "class")
final_predictions_prob <- predict(final_rf_fit, new_data = penguin_test, type = "prob")

final_results_df <- bind_cols(
  final_predictions_class,
  final_predictions_prob,
  penguin_test %>%
    select(species)
  )

final_eval_results <- classification_metrics(
  final_results_df,
  truth = species,
  estimate = .pred_class,
  .pred_Adelie, .pred_Chinstrap, .pred_Gentoo,
  estimator="macro")

final_eval_results

```

*Note: Tuning can take some time. The tuning code (`eval=FALSE`) won't be executed when rendering this notebook, but you can run it yourself.*

------------------------------------------------------------------------

## Saving and Loading Models for Future Use (`saveRDS`, `readRDS`)

Spending time training and tuning a model is a valuable investment. To avoid having to repeat this process every time you want to use the model, or to deploy it in a different environment, it's crucial to save the trained object. tidymodels doesn't have a dedicated saving function, but we can successfully use R's built-in functions like saveRDS() and readRDS().

Why save models?

**Reusability:** Avoid retraining, which can be time-consuming, especially with large datasets or complex models. **Deployment:** Use the saved model in other scripts, applications (e.g., Shiny), or production environments. **Reproducibility:** Ensure consistent results by using the exact same trained model. **Sharing:** Easily share trained models with collaborators.

The most common object to save is the fitted workflow (the result of the `fit()` function), as it contains both the prepped recipe and the fitted model.

### Step 1: Saving the Trained Model

We'll use the **saveRDS()** function to save our trained workflow `penguin_fit`. If you have performed hyperparameter tuning and have a final, fitted model (e.g., `final_rf_fit`), that would be an even better candidate for saving.

```{r}
# Assume we have the trained model 'penguin_fit'
# (or 'final_rf_fit' if the hyperparameter tuning section was executed)

# If you want to save the model after tuning, use final_rf_fit:
# saveRDS(final_rf_fit, file = "final_penguin_rf_model.rds")

# For this example, we'll save penguin_fit:
saveRDS(penguin_fit, file = "penguin_rf_model.rds")

cat("Model penguin_fit has been saved to 'penguin_rf_model.rds'\n")
```

**Eplanation:**

-   `saveRDS(object, file = "filename.rds")`: Saves a single R object (penguin_fit) to a file in the binary .rds format. This format is efficient and preserves the R object's structure.

### Step 2: Loading the Saved Model

To use the saved model in a new R session or a different script, we load it using the `readRDS()` function.

```{r}
# Load the saved model
loaded_penguin_model <- readRDS(file = "penguin_rf_model.rds")
cat("Model has been loaded from 'penguin_rf_model.rds' into 'loaded_penguin_model'\n")
```

Let's check if the loaded object is what we expect

```{r}
loaded_penguin_model
```

### Step 3: Using the Loaded Model

Once the model is loaded, we can use it to generate predictions on new data, just as if we had just trained it. Let's verify it works by making predictions on the penguin_test set.

```{r}
# Generate predictions using the loaded model
loaded_model_predictions <- predict(loaded_penguin_model, new_data = penguin_test, type = "class")


# Combine predictions with true values from the test set
loaded_results_df <- bind_cols(
  loaded_model_predictions,
  penguin_test %>% select(species)
)

head(loaded_results_df)

```

We can also evaluate its performance using the same metrics as before (assuming classification_metrics is defined earlier in the notebook) Ensure all necessary probability columns are included if your metrics (`like roc_auc`) require them

```{r}
loaded_model_prob_predictions <- predict(loaded_penguin_model, new_data = penguin_test, type = "prob")

loaded_results_with_prob_df <- bind_cols(
  loaded_model_predictions, # .pred_class
  loaded_model_prob_predictions, # .pred_Adelie, .pred_Chinstrap, .pred_Gentoo
  penguin_test %>% select(species) # true species
)

loaded_eval_results <- classification_metrics(
  loaded_results_with_prob_df,
  truth = species,
  estimate = .pred_class,
  .pred_Adelie, .pred_Chinstrap, .pred_Gentoo, # For AUC
  estimator = "macro" 
)

print(loaded_eval_results)
```

**Explanation:**

-   The loaded model `loaded_penguin_model` is a fully functional workflow object, ready for use.
-   The prediction and evaluation process is identical to that of the original, just-trained model. Note that for metrics like `roc_auc` in multiclass settings, you need to provide the probability columns (e.g., .pred_Adelie, .pred_Chinstrap, .pred_Gentoo) along with the truth and estimate (predicted class).

What Else Can Be Saved?

While saving the entire fitted workflow is most common, you can also save individual tidymodels components if needed:

Prepared recipe: If you want to re-apply the same data preprocessing steps. You can save the recipe object after it has been prepared with prep().

```{r}
# # Prepare the recipe (if not already done)
# # This 'prep_recipe' object was created in an earlier chunk.
prep_recipe <- prep(penguin_recipe, training = penguin_train) 
saveRDS(prep_recipe, "penguin_prepared_recipe.rds")
```

Model specification (parsnip): If you want to reuse the same model configuration. R

```{r}
# # This 'rf_spec' object was created in an earlier chunk.
saveRDS(rf_spec, "rf_model_specification.rds")

```

**However, saving the fitted workflow is usually the most practical approach as it bundles all necessary, prepped components.**

**Best Practices:**

-   **File Naming**: Use descriptive filenames to easily identify the model and its version (e.g., `penguin_classifier_rf_v1_20250513.rds`).
-   **Documentation**: Keep a record of what data was used for training, the model's settings, and its performance, perhaps in a text file or as comments in your script.
-   **Cleanup**: After you are done working with model files (especially if they are large), you might want to remove them to free up disk space.

------------------------------------------------------------------------

## Summary

The `tidymodels` ecosystem provides consistent and modular tools for building machine learning models in R. Key concepts include:

-   **Staged Process:** The modeling process is logically divided (data split, preprocess, model spec, workflow, train/tune, evaluate).
-   **Consistent Interface:** Packages like `parsnip` unify how models are defined, and `recipes` standardizes data preprocessing.
-   **Modularity:** Individual components (recipe, model) can be easily swapped and combined using `workflows`.
-   **Good Practices:** `tidymodels` facilitates the use of cross-validation (`rsample`, `tune`) and proper model evaluation (`yardstick`).

By following the stepsâ€”from data splitting (`rsample`), through preparation (`recipes`) and specification (`parsnip`), bundling into `workflows`, to training, tuning (`tune`), and evaluation (`yardstick`)â€”you can create robust and reliable predictive models. Understanding this philosophy and workflow is key to effectively using `tidymodels`.

**Next Steps:**

-   **Practice:** Experiment with different `step_*` functions in `recipes`, various models and engines in `parsnip`.
-   **Tuning:** Dive deeper into the capabilities of the `tune` and `dials` packages for hyperparameter optimization.
-   **Advanced Recipes:** Explore more advanced steps in `recipes`, such as `step_pca`, `step_impute_*`, `step_other`.
-   **Comparing Models:** Learn how to systematically compare the results of multiple models (e.g., using results from `fit_resamples` or `tune_grid`).
-   **Model Interpretability:** Explore packages for model explanation that work with `tidymodels`, such as `vip` (variable importance) or `DALEX`.

------------------------------------------------------------------------

## References and Additional Resources

-   **Official tidymodels website**: The best place to start, with tutorials and documentation.
    -   [tidymodels.org](https://www.tidymodels.org/)
-   **"Tidy Modeling with R" (TMwR) book**: A comprehensive guide to `tidymodels` by Max Kuhn and Julia Silge.
    -   [tmwr.org](https://www.tmwr.org/)
-   **Individual Package Documentation**: Find detailed documentation for `rsample`, `recipes`, `parsnip`, `workflows`, `tune`, `dials`, `yardstick` on the `tidymodels.org` site or CRAN.
-   **RStudio Cheat Sheets**: Look for a cheatsheet for `tidymodels`.
    -   [Posit Cheat Sheets](https://posit.co/resources/cheatsheets/) (Check if an up-to-date version is available)
